
import os
import sys
import json
import time
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv

import _path_setup
from libs.scraper import NewsScraper
from libs.utils import StateManager
import config.settings as config

def main():
    """
    ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° (News Collector)
    - íŠ¹ì • ê¸°ì—…(TARGET_COMPANY_NAME)ì— ëŒ€í•œ ìµœì‹  ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    - ìˆ˜ì§‘ëœ ë‰´ìŠ¤ëŠ” JSON íŒŒì¼ë¡œ ì €ì¥ë˜ì–´ ë‹¤ìŒ ë‹¨ê³„(Analyzer)ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤.
    """
    load_dotenv()
    
    # Environment Variables Check
    target_ticker = os.getenv("TARGET_TICKER")
    company_name = os.getenv("TARGET_COMPANY_NAME")
    
    if not company_name:
        print("âŒ Error: TARGET_COMPANY_NAME is not set in environment.")
        sys.exit(1)
        
    print(f"\nğŸ“° [News Collector] Starting news collection for: {company_name} ({target_ticker})")
    
    scraper = NewsScraper()
    state_manager = StateManager(state_file="../../data/scraping_state.json")

    # Determine time limit (Incremental Crawl)
    time_limit = state_manager.get_last_search_time_limit(company_name)
    print(f"   ğŸ•’ Search Time Limit: {time_limit}")
    
    # 1. Search Headlines
    print(f"   ğŸ” Searching for headlines...")
    # Using simple search query logic appropriate for the company
    query = f"{company_name} ì£¼ê°€ ì „ë§ ì‹¤ì  ì´ìŠˆ"
    
    # Search count from config
    search_count = getattr(config, 'NEWS_SEARCH_COUNT', 10)
    
    news_items = scraper.search_by_keyword(query, count=search_count, time_limit=time_limit)
    
    valid_news = []
    
    if news_items:
        print(f"   âœ… Found {len(news_items)} headline candidates. Scraping details...")
        
        target_scrap_count = getattr(config, 'NEWS_SCRAP_COUNT', 5)
        
        for item in news_items:
            if len(valid_news) >= target_scrap_count:
                break
                
            content = scraper.fetch_article_content(item['url'])
            if content:
                item['content'] = content 
                valid_news.append(item)
                print(f"      - Scraped: {item['title'][:30]}...")
            time.sleep(1) # Polite scraping
    else:
        print("   âš ï¸ No news found.")
        
    # Save Results
    output_dir = Path("../../data/raw_news")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    safe_name = company_name.replace(" ", "_").replace("/", "-")
    output_file = output_dir / f"{safe_name}_news.json"
    
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(valid_news, f, ensure_ascii=False, indent=4)
        
    print(f"   ğŸ’¾ Saved {len(valid_news)} news items to {output_file}")
    
    # Update State (Mark this run time)
    state_manager.update_last_run(company_name)

if __name__ == "__main__":
    main()
