import yfinance as yf
from curl_cffi import requests
from duckduckgo_search import DDGS
from bs4 import BeautifulSoup
from typing import Optional, List, Dict
import time
import random

class NewsScraper:
    def __init__(self):
        # API ì°¨ë‹¨ ëŒ€ë¹„ ì£¼ìš” ê¸°ì—… í•˜ë“œì½”ë”©
        self.known_tickers = {
            "samsung": "005930.KS", "ì‚¼ì„±ì „ì": "005930.KS",
            "nvidia": "NVDA", "ì—”ë¹„ë””ì•„": "NVDA",
            "tesla": "TSLA", "í…ŒìŠ¬ë¼": "TSLA",
            "apple": "AAPL", "ì• í”Œ": "AAPL",
            "microsoft": "MSFT", "ë§ˆì´í¬ë¡œì†Œí”„íŠ¸": "MSFT",
            "google": "GOOGL", "êµ¬ê¸€": "GOOGL",
            "skhynix": "000660.KS", "skí•˜ì´ë‹‰ìŠ¤": "000660.KS"
        }

    def fetch_article_content(self, url: str) -> Optional[str]:
        """
        URLì—ì„œ ê¸°ì‚¬ ë³¸ë¬¸ì„ ìŠ¤í¬ë˜í•‘í•©ë‹ˆë‹¤. (news_scrap_merge.py ë¡œì§ ì´ì‹)
        """
        try:
            # User Agent for bypassing basic protections
            response = requests.get(
                url, 
                impersonate="chrome110", 
                timeout=15,
                headers={"User-Agent": "Mozilla/5.0", "Referer": "https://www.google.com/"}
            )
            if response.status_code != 200: 
                return None
            
            # Domain filtering
            skip_domains = ["namu.wiki", "samsung.com", "sec.co.kr", "ko.wikipedia.org", "youtube.com"]
            if any(d in url for d in skip_domains):
                return None

            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Remove distracting tags
            for tag in soup(["script", "style", "nav", "footer", "header", "iframe", "aside"]):
                tag.decompose()
                
            # Heuristic to find article body (Compatible with Investing.com & General sites)
            article_body = soup.find('div', class_='WYSIWYG articlePage') or \
                           soup.find('div', class_='article_container') or \
                           soup.find('div', id='article-content')
            
            target = article_body if article_body else soup
            paragraphs = target.find_all('p')
            
            # Join paragraphs usually containing text
            content = "\n\n".join([p.get_text().strip() for p in paragraphs if len(p.get_text()) > 30])
            
            return content if len(content) > 100 else None
            
        except Exception as e:
            print(f"      âŒ Scraping failed: {e}")
            return None

    def search_deep_news(self, ticker: str, keyword: str, count: int = 5) -> List[Dict[str, str]]:
        """
        DuckDuckGoë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • í‚¤ì›Œë“œì— ëŒ€í•œ ì‹¬ì¸µ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        """
        import config
        query = f"{ticker} {keyword} news"
        print(f"ğŸ” Deep Searching: '{query}'...")
        
        try:
            region = getattr(config, 'NEWS_SEARCH_REGION', 'wt-wt')
            results = DDGS().news(keywords=query, max_results=count, region=region)
            news_list = []
            if results:
                for res in results:
                    news_list.append({
                        "title": res.get("title", "No Title"),
                        "url": res.get("url", "#"),
                        "source": res.get("source", "Unknown"),
                        "date": res.get("date", "")
                    })
            print(f"   âœ… Found {len(news_list)} articles for '{keyword}'")
            return news_list
        except Exception as e:
            print(f"   âš ï¸ Search failed for '{keyword}': {e}")
            return []

    def search_by_keyword(self, keyword: str, count: int = 5, time_limit: str = None, region_key: str = 'MACRO_SEARCH_REGION') -> List[Dict[str, str]]:
        """
        íŠ¹ì • í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ê²€ìƒ‰ (Macroìš©). region_key ì„¤ì •ê°’('MACRO' or 'NEWS') ì‚¬ìš©.
        """
        import config # Lazy import to avoid circular dependency if any
        print(f"ğŸ” Keyword Searching: '{keyword}'...")
        try:
            region = getattr(config, region_key, 'wt-wt')
            results = DDGS().news(keywords=keyword, max_results=count, region=region, timelimit=time_limit)
            
            news_list = []
            if results:
                for res in results:
                     news_list.append({
                        "title": res.get("title", "No Title"),
                        "url": res.get("url", "#"),
                        "source": res.get("source", "Unknown"),
                        "date": res.get("date", "")
                    })
            print(f"   âœ… Found {len(news_list)} articles for '{keyword}'")
            return news_list
        except Exception as e:
            print(f"   âš ï¸ Search failed for '{keyword}': {e}")
            return []

    def get_ticker(self, query: str) -> Optional[str]:
        """
        ê¸°ì—…ëª…ì„ ì…ë ¥ë°›ì•„ Ticker Symbolì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        1. í•˜ë“œì½”ë”©ëœ ë¦¬ìŠ¤íŠ¸ í™•ì¸
        2. Yahoo Finance Search API ì‚¬ìš©
        """
        query_lower = query.lower().strip()
        
        # 1. í•˜ë“œì½”ë”© ë§¤í•‘ í™•ì¸
        if query_lower in self.known_tickers:
            print(f"âœ… Known ticker found: {query} -> {self.known_tickers[query_lower]}")
            return self.known_tickers[query_lower]

        # 2. í‹°ì»¤ í˜•ì‹ì´ë©´ ë°”ë¡œ ë°˜í™˜ (ê°„ë‹¨í•œ ì²´í¬)
        if query.upper() in ["NVDA", "AAPL", "TSLA", "MSFT", "GOOG", "AMZN"]:
            return query.upper()
        if query.endswith(".KS") or query.endswith(".KQ"):
            return query.upper()

        # 3. Yahoo Finance API ê²€ìƒ‰
        print(f"ğŸ” Searching ticker for: {query}...")
        url = "https://query2.finance.yahoo.com/v1/finance/search"
        headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }
        params = {"q": query, "quotesCount": 1, "newsCount": 0}

        try:
            res = requests.get(url, params=params, headers=headers)
            data = res.json()
            if 'quotes' in data and len(data['quotes']) > 0:
                symbol = data['quotes'][0]['symbol']
                print(f"âœ… Found via API: {symbol}")
                return symbol
        except Exception as e:
            print(f"âš ï¸ Ticker search failed: {e}")
        
        return None

    def get_headlines(self, ticker_symbol: str, count: int = 20) -> List[str]:
        """
        í•´ë‹¹ í‹°ì»¤ì˜ ìµœì‹  ë‰´ìŠ¤ í—¤ë“œë¼ì¸ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        """
        print(f"ğŸ“¡ Fetching headlines for: {ticker_symbol}")
        try:
            ticker = yf.Ticker(ticker_symbol)
            news = ticker.news
            
            headlines = []
            if news:
                for item in news[:count]:
                    # Handle varying yfinance news structure
                    title = item.get('title')
                    if not title and 'content' in item:
                        title = item['content'].get('title')
                    
                    if title:
                        headlines.append(title)
            
            print(f"âœ… Retrieved {len(headlines)} headlines.")
            return headlines
        except Exception as e:
            print(f"âŒ Error fetching headlines: {e}")
            return []

    def _fetch_google_rss(self, query: str) -> List[str]:
        """Google News RSS Fallback"""
        try:
            import urllib.parse
            encoded_query = urllib.parse.quote(query)
            # Google News RSS (Korean edition)
            url = f"https://news.google.com/rss/search?q={encoded_query}&hl=ko&gl=KR&ceid=KR:ko"
            
            response = requests.get(url, timeout=10)
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, features='xml')
                items = soup.find_all('item')
                return [item.title.text for item in items[:10] if item.title]
        except Exception as e:
            print(f"      âŒ RSS Error for '{query}': {e}")
        return []

    def get_macro_headlines(self, seed_queries: List[str], time_limit: str = 'w') -> List[str]:
        """
        ì£¼ì–´ì§„ ì‹œë“œ ì¿¼ë¦¬ë¡œ í—¤ë“œë¼ì¸ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤. time_limit('d', 'w', 'm') ë°˜ì˜.
        """
        import config # Lazy import to avoid circular dependency
        print(f"ğŸŒ Fetching Macro Headlines ({time_limit}) Region: {getattr(config, 'MACRO_SEARCH_REGION', 'wt-wt')}...")
        headlines = set()
        
        try:
            for query in seed_queries:
                print(f"   ğŸ” Scanning: {query}...")
                try:
                    # Apply time_limit to DuckDuckGo search
                    # config.MACRO_SEARCH_REGION ì‚¬ìš© (ê¸°ë³¸ê°’: 'wt-wt')
                    region = getattr(config, 'MACRO_SEARCH_REGION', 'wt-wt')
                    results = DDGS().news(keywords=query, max_results=10, region=region, timelimit=time_limit)
                    if results:
                        for item in results:
                            headlines.add(item.get('title'))
                    time.sleep(3) 
                except Exception as inner_e:
                    print(f"      âš ï¸ Failed to fetch for '{query}': {inner_e}")
                    time.sleep(5) 
 
                
            # Fallback Check
            if len(headlines) < 5:
                print("\n   âš ï¸ DDGS yielded few results. Activating Google RSS Fallback...")
                for query in seed_queries:
                    print(f"   ğŸ“¡ RSS Scanning: {query}...")
                    rss_titles = self._fetch_google_rss(query)
                    headlines.update(rss_titles)
                    time.sleep(1)

            headline_list = list(headlines)
            print(f"âœ… Collected {len(headline_list)} unique macro headlines.")
            return headline_list
        except Exception as e:
            print(f"âŒ Error fetching macro headlines: {e}")
            return []
