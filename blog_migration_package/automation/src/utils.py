import os
import re
import json
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional

class StateManager:
    """
    ë‰´ìŠ¤ ìˆ˜ì§‘ ìƒíƒœ(ë§ˆì§€ë§‰ ì‹¤í–‰ì¼, ìˆ˜ì§‘ëœ URL)ë¥¼ ê´€ë¦¬í•˜ì—¬ ì¤‘ë³µ ìˆ˜ì§‘ì„ ë°©ì§€í•©ë‹ˆë‹¤.
    """
    def __init__(self, state_file="data/scraping_state.json"):
        self.state_file = Path(state_file)
        self.state_file.parent.mkdir(parents=True, exist_ok=True)
        self.state = self._load_state()

    def _load_state(self) -> Dict:
        if self.state_file.exists():
            try:
                with open(self.state_file, "r", encoding="utf-8") as f:
                    return json.load(f)
            except:
                return {}
        return {}

    def save_state(self):
        with open(self.state_file, "w", encoding="utf-8") as f:
            json.dump(self.state, f, indent=4)

    def get_last_search_time_limit(self, keyword: str) -> str:
        """
        ë§ˆì§€ë§‰ ìˆ˜ì§‘ì¼ì„ ê¸°ì¤€ìœ¼ë¡œ ê²€ìƒ‰ ê¸°ê°„ ì˜µì…˜('d', 'w', 'm')ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        last_date_str = self.state.get("last_run", {}).get(keyword)
        if not last_date_str:
            return 'm' # ì²˜ìŒì´ë©´ í•œ ë‹¬ ì¹˜ ê²€ìƒ‰
        
        last_date = datetime.strptime(last_date_str, "%Y-%m-%d")
        delta = (datetime.now() - last_date).days
        
        if delta <= 1:
            return 'd' # ì–´ì œ ì´í›„ë©´ 1ì¼ ì¹˜
        elif delta <= 7:
            return 'w' # 1ì£¼ì¼ ì´ë‚´ë©´ 1ì£¼ ì¹˜
        else:
            return 'm'

    def update_last_run(self, keyword: str):
        if "last_run" not in self.state:
            self.state["last_run"] = {}
        self.state["last_run"][keyword] = datetime.now().strftime("%Y-%m-%d")
        self.save_state()

class ReportGenerator:
    def __init__(self, base_dir="scraped_news"):
        self.base_dir = Path(base_dir)
        
    def sanitize_filename(self, text: str) -> str:
        # Replace forbidden characters with underscore
        return re.sub(r'[\\/*?:"<>| ]', '_', text)

    def save_analysis_report(self, report_name: str, content: str):
        """
        AI ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ 'analysis_result/YYYY-MM-DD/' í´ë”ì— ì €ì¥í•©ë‹ˆë‹¤.
        """
        today_str = datetime.now().strftime("%Y-%m-%d")
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥ ê²½ë¡œ: analysis_result/YYYY-MM-DD/
        save_dir = Path("analysis_result") / today_str
        save_dir.mkdir(parents=True, exist_ok=True)
        
        filename = f"{today_str}_{report_name}.md"
        file_path = save_dir / filename
        
        try:
            with open(file_path, "w", encoding="utf-8") as f:
                f.write(content)
            print(f"âœ… Analysis Report Saved: {file_path}")
            return str(file_path)
        except Exception as e:
            print(f"âŒ Failed to save analysis report: {e}")
            return None

    def save_consolidated_report(self, title: str, news_data: Dict[str, List[Dict[str, str]]]):
        """
        ì—¬ëŸ¬ í‚¤ì›Œë“œì˜ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ íŒŒì¼ë¡œ í†µí•© ì €ì¥í•©ë‹ˆë‹¤. (ê±°ì‹œ ê²½ì œìš©)
        news_data êµ¬ì¡°: { "í‚¤ì›Œë“œ": [ë‰´ìŠ¤í•­ëª©1, ë‰´ìŠ¤í•­ëª©2...] }
        """
        today_str = datetime.now().strftime("%Y-%m-%d")
        
        # ë””ë ‰í† ë¦¬ ìƒì„± (ì˜ˆ: scraped_news/MACRO_ECONOMY/)
        self.base_dir.mkdir(parents=True, exist_ok=True)
        
        # íŒŒì¼ëª… ìƒì„±
        filename = f"{today_str}_{title}.md"
        file_path = self.base_dir / filename
        
        content = [
            f"# ğŸŒ {title.replace('_', ' ')}",
            f"**Date**: {today_str}",
            "---"
        ]
        
        for topic, items in news_data.items():
            content.append(f"## ğŸ“Œ Topic: {topic}")
            content.append(f"*(Found {len(items)} articles)*")
            content.append("")
            
            for idx, item in enumerate(items, 1):
                raw_content = item.get('content', 'No Content Available.')
                # Format content with blockquotes for better readability
                formatted_content = "\n".join([f"> {line}" for line in raw_content.split('\n') if line.strip()])
                
                content.append(f"### {idx}. [{item['title']}]({item['url']})")
                content.append(f"**Source**: {item.get('source', 'Unknown')} | **Date**: {item.get('date', '-')}")
                content.append("")
                content.append(formatted_content)
                content.append("")
                content.append("---")
                content.append("")
            
        try:
            with open(file_path, "w", encoding="utf-8") as f:
                f.write("\n".join(content))
            print(f"   ğŸ’¾ Consolidated Report Saved: {file_path}")
        except Exception as e:
            print(f"   âŒ Failed to save consolidated report: {e}")

    def save_report(self, ticker: str, keyword: str, news_items: List[Dict[str, str]]):
        """
        ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ë¥¼ Markdown íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
        íŒŒì¼ëª… í˜•ì‹: ë‚ ì§œ_í‹°ì»¤_í‚¤ì›Œë“œ.md (ì˜ˆ: 2026-02-02_NVDA_AI_Chip.md)
        """
        if not news_items:
            return

        # 1. Directory Setup
        today_str = datetime.now().strftime("%Y-%m-%d")
        ticker_dir = self.base_dir / ticker
        ticker_dir.mkdir(parents=True, exist_ok=True)
        
        # 2. Filename Construction
        safe_keyword = self.sanitize_filename(keyword)
        filename = f"{today_str}_{ticker}_{safe_keyword}.md"
        file_path = ticker_dir / filename
        
        # 3. Content Generation
        content = [
            f"# ğŸ“Š Deep Dive Report: {keyword}",
            f"**Target**: {ticker}",
            f"**Date**: {today_str}",
            f"**Source**: DuckDuckGo News Search",
            "",
            "## ğŸ“° Key News Articles",
            ""
        ]
        
        for idx, item in enumerate(news_items, 1):
            content.append(f"### {idx}. [{item['title']}]({item['url']})")
            content.append(f"**Source**: {item['source']} | **Date**: {item['date']}")
            content.append("")
            content.append(item.get('content', '*Content could not be scraped.*'))
            content.append("")
            content.append("---")
            content.append("")
        
        content.append("Generated by Smart News Analyzer ğŸ¤–")

        # 4. Save File
        try:
            with open(file_path, "w", encoding="utf-8") as f:
                f.write("\n".join(content))
            print(f"   ğŸ’¾ Report Saved: {file_path}")
        except Exception as e:
            print(f"   âŒ Failed to save report: {e}")

    def save_final_report(self, ticker: str, report_content: str):
        """
        ìµœì¢… íˆ¬ì ë¸Œë¦¬í•‘ ë¦¬í¬íŠ¸ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
        """
        today_str = datetime.now().strftime("%Y-%m-%d")
        ticker_dir = self.base_dir / ticker
        ticker_dir.mkdir(parents=True, exist_ok=True)
        
        filename = f"{today_str}_{ticker}_Daily_Investment_Briefing.md"
        file_path = ticker_dir / filename
        
        try:
            with open(file_path, "w", encoding="utf-8") as f:
                f.write(report_content)
            print(f"ğŸ“ Final Investment Briefing saved to: {file_path}")
        except Exception as e:
            print(f"âŒ Failed to save final report: {e}")
